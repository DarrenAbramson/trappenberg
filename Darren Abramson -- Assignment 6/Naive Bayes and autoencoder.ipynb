{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "The following work is based on the tutorial located at [http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). First, import the dataset and assign a variable to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some print statements to get a sense of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "11314\n",
      "11314\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "rec.autos\n",
      "comp.sys.mac.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.graphics\n",
      "sci.space\n",
      "talk.politics.guns\n",
      "sci.med\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "# List of names corresponding to newsgroup integers\n",
    "print twenty_train.target_names\n",
    "# List of emails\n",
    "print len(twenty_train.data)\n",
    "# List of newsgroup integer \n",
    "print len(twenty_train.target)\n",
    "# Print the first three lines of the 0th email\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "# Print the newsgroups corresponding to the target integers of the first 10 emails\n",
    "for t in twenty_train.target[:10]:print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: computationally expensive. The built in vectorizer creates a sparse representation of words that are longer than 2 letters, removing stop words (highly common words) and punctuation. See e.g. [http://scikit-learn.org/stable/modules/feature_extraction.html](http://scikit-learn.org/stable/modules/feature_extraction.html). By setting `binary=True` the vectorizer considers all non-zero counts to be 1. This corresponds to the binomial version of the naive Bayes assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer to convert the returned object to a simple array. The `CountVectorizer.build_analyzer()` is a useful tool for extracting feature indices from test data. A sample text is analyzed and given as a list of words, and as a list of feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "(11314, 130107)\n",
      "[u'this', u'is', u'something', u'that', u'you', u'never', u'saw', u'but', u'might', u'want', u'to', u'categorize']\n",
      "[114731, 68532, 108821, 114440, 128402, 86839, 104813, 35805, 81998, 123196, 115475, 38131]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_counts.toarray()\n",
    "print X_train[47]\n",
    "print X_train.shape\n",
    "analyze = count_vect.build_analyzer()\n",
    "a = analyze(\"This is something that you never saw, but might want to categorize.\")\n",
    "print a\n",
    "indices = []\n",
    "for item in a:\n",
    "    indices.append(count_vect.vocabulary_.get(item))\n",
    "print indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that features for some random word are indeed binomial. In the following example, the 3453th word contains 63 unique words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[130044     63]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(X_train[3453], return_counts=True)\n",
    "print unique\n",
    "print counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to use this to check whether the count is non-zero. My first guess of a non-occurring word was successful!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print type(count_vect.vocabulary_.get('doogie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how balanced the data is across newsgroups. Looks quite balanced. A sanity check is provided to confirm a partition of categories in the provided list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism : (480, 130107)\n",
      "comp.graphics : (584, 130107)\n",
      "comp.os.ms-windows.misc : (591, 130107)\n",
      "comp.sys.ibm.pc.hardware : (590, 130107)\n",
      "comp.sys.mac.hardware : (578, 130107)\n",
      "comp.windows.x : (593, 130107)\n",
      "misc.forsale : (585, 130107)\n",
      "rec.autos : (594, 130107)\n",
      "rec.motorcycles : (598, 130107)\n",
      "rec.sport.baseball : (597, 130107)\n",
      "rec.sport.hockey : (600, 130107)\n",
      "sci.crypt : (595, 130107)\n",
      "sci.electronics : (591, 130107)\n",
      "sci.med : (594, 130107)\n",
      "sci.space : (593, 130107)\n",
      "soc.religion.christian : (599, 130107)\n",
      "talk.politics.guns : (546, 130107)\n",
      "talk.politics.mideast : (564, 130107)\n",
      "talk.politics.misc : (465, 130107)\n",
      "talk.religion.misc : (377, 130107)\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "totalEntries = 0\n",
    "for i in range(len(twenty_train.target_names)):\n",
    "    boolVec = twenty_train.target == i\n",
    "    #print boolVec[0:47]\n",
    "    subset = X_train[boolVec,:]\n",
    "    totalEntries += subset.shape[0]\n",
    "    print twenty_train.target_names[i], \":\", subset.shape\n",
    "\n",
    "print totalEntries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a word and an integer category, it calculates the likelihood that a document with that word belongs\n",
    "# to that category.\n",
    "\n",
    "# Subset the training dataset for the category.\n",
    "# Numerator:    number of occurrences of word + 1\n",
    "# Denominator:  number of documents in subset +  len(twenty_train.target_names)\n",
    "from itertools import compress\n",
    "\n",
    "totalDocs = X_train.shape[0]\n",
    "subset = []\n",
    "def wordCatProb(word):\n",
    "    #subsetBoolVector = twenty_train.target == category\n",
    "    #subset = X_train[subsetBoolVector,:]\n",
    "    index = count_vect.vocabulary_.get(word)\n",
    "    if index is None:\n",
    "        numerator = float(1)\n",
    "    else:\n",
    "        numerator = float(sum(subset[:,index])) + 1\n",
    "    denominator = subset.shape[0] + len(twenty_train.target_names)\n",
    "    return numerator/denominator\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that just over one third of the entries in the \"Atheism\" newsgroup contain the word \"God\" or \"god\", whereas soc.religion.christian has that word in just over half of its submissions. Note that the vectorizer converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurences of god in alt.atheism : 0.346\n",
      "Occurences of god in soc.religion.christian : 0.557350565428\n"
     ]
    }
   ],
   "source": [
    "word = \"god\"\n",
    "print \"Occurences of\", word, \"in\", twenty_train.target_names[0],\":\",  wordCatProb(word, 0)\n",
    "print \"Occurences of\", word, \"in\", twenty_train.target_names[15],\":\",  wordCatProb(word, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n",
      "rec.autos\n",
      "[u'from', u'v064mb9k', u'ubvmsd', u'cc', u'buffalo', u'edu', u'neil', u'gandler', u'subject', u'need', u'info', u'on', u'88', u'89', u'bonneville', u'organization', u'university', u'at', u'buffalo', u'lines', u'10', u'news', u'software', u'vax', u'vms', u'vnews', u'41', u'nntp', u'posting', u'host', u'ubvmsd', u'cc', u'buffalo', u'edu', u'am', u'little', u'confused', u'on', u'all', u'of', u'the', u'models', u'of', u'the', u'88', u'89', u'bonnevilles', u'have', u'heard', u'of', u'the', u'le', u'se', u'lse', u'sse', u'ssei', u'could', u'someone', u'tell', u'me', u'the', u'differences', u'are', u'far', u'as', u'features', u'or', u'performance', u'am', u'also', u'curious', u'to', u'know', u'what', u'the', u'book', u'value', u'is', u'for', u'prefereably', u'the', u'89', u'model', u'and', u'how', u'much', u'less', u'than', u'book', u'value', u'can', u'you', u'usually', u'get', u'them', u'for', u'in', u'other', u'words', u'how', u'much', u'are', u'they', u'in', u'demand', u'this', u'time', u'of', u'year', u'have', u'heard', u'that', u'the', u'mid', u'spring', u'early', u'summer', u'is', u'the', u'best', u'time', u'to', u'buy', u'neil', u'gandler']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "test_emails = twenty_test.data\n",
    "test_labels = twenty_test.target\n",
    "\n",
    "print test_emails[0]\n",
    "print twenty_train.target_names[test_labels[0]]\n",
    "print analyze(test_emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "test_predictions = []\n",
    "counter = 0\n",
    "\n",
    "test_emails = test_emails[0:400]\n",
    "\n",
    "for i in test_emails:\n",
    "    t0 = time()\n",
    "    squozenEmail = analyze(i)\n",
    "    squozenEmail = set(squozenEmail)\n",
    "    phi = []\n",
    "    probDict = {}\n",
    "    for cat in range(len(twenty_train.target_names)):\n",
    "        subsetBoolVector = twenty_train.target == cat\n",
    "        subset = X_train[subsetBoolVector,:]\n",
    "        # cat is the Category under consideration.\n",
    "        prob = 1\n",
    "        for word in squozenEmail:\n",
    "            if word + str(cat) in probDict.keys():\n",
    "                prob *= probDict[word + str(cat)]\n",
    "            else:\n",
    "            #print word\n",
    "                curProb = wordCatProb(word)\n",
    "                prob *= curProb\n",
    "                probDict[word + str(cat)] = curProb\n",
    "            #print \"wordcatprob\", wordCatProb(word)\n",
    "        phi.append(prob)\n",
    "        #print prob\n",
    "    test_predictions.append(phi.index(max(phi)))\n",
    "    if counter % 10 == 0:\n",
    "        print \"Just finished email number\", \"\\t\\t\", counter\n",
    "        print \"Category was\", \"\\t\\t\", test_predictions[counter]\n",
    "        print \"Categorization time:\", \"\\t\\t\", round(time()-t0, 3), \"s\"\n",
    "    counter+=1 \n",
    "    \n",
    "print test_predictions.shape     \n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  5  0 17 19 13 15 15  5  1  2  5 17  8  0  2  4  1  6 16]\n"
     ]
    }
   ],
   "source": [
    "print test_labels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
