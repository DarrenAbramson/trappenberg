{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darren Abramson\n",
    "## Assignment 3\n",
    "\n",
    "### 1. Predict the class label and discuss how good your prediction is.\n",
    "\n",
    "First, load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "data = np.loadtxt(\"train.txt\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `sklearn` conventions it is convenient to transpose the data so that columns represent features and rows represent data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 11)\n"
     ]
    }
   ],
   "source": [
    "data = np.transpose(data)\n",
    "print data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and labels are separated out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "labels = data[:, 10]\n",
    "print labels.shape\n",
    "features = data[:, 0:10:1]\n",
    "print features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we examine how balanced the labels are between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.   20.]\n",
      " [   1.  180.]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This presents an immediate difficulty. If we assign, say, 30% of the data randomly to a test set, there will only be about 6 items from class `0` in the training set. This will affect our ability to assess the quality of the model. Nevertheless, for the purpose of this exercise, I will use this split recognizing that sophisticated exist (e.g. repeating the less-represented class label, with or without some level of noise).\n",
    "\n",
    "Below I fit a simple classifier using cross-validation and examine its accuracy on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation performance:  0.821428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "clf = gnb\n",
    "\n",
    "from sklearn import cross_validation\n",
    "k_fold = cross_validation.KFold(len(features_train), 10)\n",
    "\n",
    "averagePerformance = 0.\n",
    "\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    clf.fit(features_train[train], labels_train[train])\n",
    "    #print(k, clf.score(features_train[test], labels_train[test]))\n",
    "    averagePerformance += clf.score(features_train[test], labels_train[test])\n",
    "\n",
    "print \"Average cross-validation performance: \" , averagePerformance / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5]\n",
      " [ 5 49]]\n",
      "accuracy score for classifier with 30% withheld test set: \t0.833333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "print confusion_matrix(labels_test, pred)\n",
    "print \"accuracy score for classifier with 30% withheld test set:\", \"\\t\", accuracy_score(labels_test, pred)\n",
    "\n",
    "print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **very bad** model. It classified the 0 label correctly only once, and misclassified it 10 times (there are equal numbers of false positives and false negatives of the 0 label). Furthermore, its overall performance is worse than a classifier that assigns the value of 1 to every input. \n",
    "\n",
    "Assuming the incidence of 0 and 1 labels in the available data is close to their actual frequencies, such a classifier would achieve 90% accuracy; this model has an accuracy of only 83.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the maximum likelihood estimate of the parameter $\\phi$ from $m$ independent trials in which $h$ heads are thrown given a Bernoulli random variable $Y$?\n",
    "\n",
    "A Bernoulli random variable $Y$ is characterized by the given probability $P(Y=y) = \\phi^y(1-\\phi)^{1-y}$. To derive the maximum likelihood estimate of the parameter $\\phi$ from $m$ independent trials in which $h$ heads are thrown, we write the log likelihood and set its derivative to 0 to find the maximum.\n",
    "\n",
    "Let $y_h = h/m$ be the proportion of trials that are heads.\n",
    "\n",
    "$$log\\, P (Y=h) = log\\, \\phi^{y_h}(1-\\phi)^{1-y_h}$$\n",
    "\n",
    "$$ = y_h\\, log\\,\\phi + (1-y_h)\\,log\\,(1-\\phi)$$\n",
    "\n",
    "Now we find the partial derivative with respect to $\\phi$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\phi} y_h\\, log\\,\\phi + (1-y_h)\\,log\\,(1-\\phi) = \\frac{\\phi - y_h}{(\\phi-1)\\phi}$$\n",
    "\n",
    "[1]: http://www.wolframalpha.com/input/?i=d%2Fdx+y+log+x+%2B+(1-y)log(1-x)\n",
    "\n",
    "Note: due to my rusty calculus, the above line was the result of visiting [Wolfram Alpha][1] (substituting $h$ for $y$ and $\\phi$ for $x$).\n",
    "\n",
    "Setting the partial derivative gives $\\phi = y_h = h/m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Factorize the joint probability $p(x_1, x_2, x_3, x_4)$ according to the provided graphical causal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Trappenberg (201x) illustrates with an example (Equation 4.24), a causal directed acyclic graph depicts causal dependency that provides a factorization of joint probability that is simpler than the result of applying the general chain rule. In this case, the joint probability is \n",
    "\n",
    "$$P(x_1) P(x_2) P(x_3 \\mid x_1, x_2) P(x_4 \\mid x_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Write out the network as a function $x_4 = \\ldots$ and give the value for input $x_1 = 1, x_2 = 2$.\n",
    "\n",
    "I define a python function that computes the output of the specified neural network below along with its output on the given input values. I have included the $w_{x_3 x_1}$ weight for clarity even though it is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995052065576\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def smallNN(x1, x2):\n",
    "    return math.tanh(3. * math.tanh(x1 * 1. + x2 * 2))\n",
    "\n",
    "print smallNN(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
